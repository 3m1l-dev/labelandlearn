{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$NewAIRDROP WORLDâ€™S FIRST HYPERLOCAL BLOCKCHAIN PROTOCOL 1000x+ Faster than Dash,Ethereum & Bitcoin(in order). Truly democratic(No hogging or polluting of PoS/PoW). Dynamic zones for global continuity https://www.talking.im/airdrop/2yhvbc7sGSÂ â€¦\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"old/5001labelled.csv\")\n",
    "label = df[['useful', 'text']]\n",
    "print(label.iloc[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'newairdrop world first hyperlocal blockchain protocol 1000x faster dash ethereum bitcoin order truly democratic hogging polluting pos pow dynamic zones global continuity')\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def format_text(s):\n",
    "    s = re.sub(r\"http\\S+\", \"\", s)\n",
    "    s = re.sub('[^0-9a-z #+_]', ' ', s.lower());\n",
    "    s = \" \".join(word for word in s.split() if word not in STOPWORDS)\n",
    "    return s\n",
    "\n",
    "label = label[label[\"text\"].notnull()]\n",
    "label.loc[:,\"text\"] = label.text.apply(lambda x: format_text(x))\n",
    "label.loc[:, \"text\"] = label.text.apply(lambda x : \" \".join(re.findall('[\\w]+'\n",
    "         ,x)))\n",
    "#label[\"text\"] = label[\"text\"].str.lower()\n",
    "training = [tuple(x) for x in label.values]\n",
    "\n",
    "print(training[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "731\n",
      "4269\n"
     ]
    }
   ],
   "source": [
    "# create our training data from the tweets\n",
    "train_x = np.asarray([x[1] for x in training])\n",
    "# index all the sentiment labels\n",
    "train_y = np.asarray([x[0] for x in training])\n",
    "\n",
    "# only work with the 3000 most popular words found in our dataset\n",
    "max_words = 10000\n",
    "\n",
    "# print(train_x[0])\n",
    "# print(train_y[0])\n",
    "\n",
    "useful_examples_index = np.where(train_y > 0)[0]\n",
    "number_of_useful_examples = len(useful_examples_index)\n",
    "useless_examples_index = np.where(train_y == 0)[0]\n",
    "number_of_useless_examples = len(useless_examples_index)\n",
    "\n",
    "print(number_of_useful_examples)\n",
    "print(number_of_useless_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Tokenizer\n",
    "tknzr = Tokenizer(lower=True, split=\" \", num_words=max_words)\n",
    "tknzr.fit_on_texts(train_x)\n",
    "\n",
    "#vocabulary:\n",
    "# print(tknzr.word_index)\n",
    "\n",
    "tokenized_train_x = tknzr.texts_to_sequences(train_x)\n",
    "\n",
    "#remove duplicate tokens\n",
    "for i in range(0, len(tokenized_train_x)):\n",
    "    tokenized_train_x[i] = list(set(tokenized_train_x[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  1. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "training_vectors = np.zeros((len(tokenized_train_x), max_words))\n",
    "# create one-hot matrices out of the indexed tweets\n",
    "for i in range(0, len(tokenized_train_x)):\n",
    "      training_vectors[i][tokenized_train_x[i]] = 1\n",
    "\n",
    "print(training_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4050 samples, validate on 450 samples\n",
      "Epoch 1/10\n",
      "4050/4050 [==============================] - 9s 2ms/step - loss: 0.4001 - acc: 0.8484 - val_loss: 0.3016 - val_acc: 0.8689\n",
      "Epoch 2/10\n",
      "4050/4050 [==============================] - 8s 2ms/step - loss: 0.2379 - acc: 0.8783 - val_loss: 0.3047 - val_acc: 0.8778\n",
      "Epoch 3/10\n",
      "4050/4050 [==============================] - 8s 2ms/step - loss: 0.0951 - acc: 0.9637 - val_loss: 0.4169 - val_acc: 0.8667\n",
      "Epoch 4/10\n",
      "4050/4050 [==============================] - 10s 2ms/step - loss: 0.0379 - acc: 0.9879 - val_loss: 0.5090 - val_acc: 0.8889\n",
      "Epoch 5/10\n",
      "4050/4050 [==============================] - 9s 2ms/step - loss: 0.0204 - acc: 0.9936 - val_loss: 0.5484 - val_acc: 0.8822\n",
      "Epoch 6/10\n",
      "4050/4050 [==============================] - 8s 2ms/step - loss: 0.0181 - acc: 0.9941 - val_loss: 0.5828 - val_acc: 0.8822\n",
      "Epoch 7/10\n",
      "4050/4050 [==============================] - 8s 2ms/step - loss: 0.0133 - acc: 0.9953 - val_loss: 0.6417 - val_acc: 0.8867\n",
      "Epoch 8/10\n",
      "4050/4050 [==============================] - 8s 2ms/step - loss: 0.0152 - acc: 0.9943 - val_loss: 0.6246 - val_acc: 0.8867\n",
      "Epoch 9/10\n",
      "4050/4050 [==============================] - 8s 2ms/step - loss: 0.0102 - acc: 0.9963 - val_loss: 0.6357 - val_acc: 0.8800\n",
      "Epoch 10/10\n",
      "4050/4050 [==============================] - 8s 2ms/step - loss: 0.0104 - acc: 0.9956 - val_loss: 0.6564 - val_acc: 0.8778\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# treat the labels as categories\n",
    "# train_y = keras.utils.to_categorical(train_y, 2)\n",
    "\n",
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=max_words, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# evaluate model with standardized dataset\n",
    "# estimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=1)\n",
    "# kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "# results = cross_val_score(estimator, training_vectors, train_y, cv=kfold)\n",
    "# print(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "\n",
    "\n",
    "model = create_baseline()\n",
    "\n",
    "model.fit(training_vectors[:4500], train_y[:4500],\n",
    "  batch_size=32,\n",
    "  epochs=10,\n",
    "  verbose=1,\n",
    "  validation_split=0.1,\n",
    "  shuffle=True)\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model on Test Set\n",
      "Number of spam tweets: 414 Number of useful tweets: 86\n",
      "Accuracy on test set:  0.854\n",
      "Identified 0.36046511627906974 of useful tweets\n",
      "Identified 0.9565217391304348 of spam tweets\n"
     ]
    }
   ],
   "source": [
    "#use completely unseen data (untrained)\n",
    "\n",
    "print(\"Running Model on Test Set\")\n",
    "\n",
    "prediction = model.predict(training_vectors[4500:]) \n",
    "actual_y = train_y[4500:]\n",
    "total = len(actual_y)\n",
    "correct = 0\n",
    "useful = 0\n",
    "spam = 0\n",
    "\n",
    "actual_useful = np.count_nonzero(actual_y)\n",
    "actual_spam = len(actual_y) - actual_useful\n",
    "\n",
    "print(\"Number of spam tweets: \" + str(actual_spam) + \" Number of useful tweets: \" + str(actual_useful))\n",
    "\n",
    "for p in range(0, len(prediction)):\n",
    "    predicted = round(prediction[p][0])\n",
    "    if predicted == actual_y[p]:\n",
    "        correct += 1\n",
    "        if predicted == 1:\n",
    "            useful += 1\n",
    "        if predicted == 0:\n",
    "            spam += 1\n",
    "        \n",
    "print(\"Accuracy on test set:  \" + str(correct/total))\n",
    "print(\"Identified \" + str(useful/actual_useful) + \" of useful tweets\")\n",
    "print(\"Identified \" + str(spam/actual_spam) + \" of spam tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(filename, data_y, data_x):\n",
    "    #create new dataframe\n",
    "    df = pd.read_csv(filename)\n",
    "    data = df[[data_y, data_x]]\n",
    "    data = data[label[data_x].notnull()]\n",
    "    data.loc[:,data_x] = data.text.apply(lambda x: format_text(x))\n",
    "    data.loc[:, data_x] = data.text.apply(lambda x : \" \".join(re.findall('[\\w]+'\n",
    "             ,x)))\n",
    "    \n",
    "    # training features\n",
    "    training = [tuple(x) for x in data.values]\n",
    "    train_x = np.asarray([x[1] for x in training])\n",
    "    \n",
    "    # tokenize\n",
    "    tknzr.fit_on_texts(train_x)\n",
    "    tokenized_train_x = tknzr.texts_to_sequences(train_x)\n",
    "    #remove duplicate tokens\n",
    "    for i in range(0, len(tokenized_train_x)):\n",
    "        tokenized_train_x[i] = list(set(tokenized_train_x[i]))\n",
    "        \n",
    "    # training vectors\n",
    "    training_vectors = np.zeros((len(tokenized_train_x), max_words))\n",
    "    # create one-hot matrices out of the indexed tweets\n",
    "    for i in range(0, len(tokenized_train_x)):\n",
    "          training_vectors[i][tokenized_train_x[i]] = 1\n",
    "    \n",
    "    \n",
    "    return data, training_vectors\n",
    "    \n",
    "\n",
    "data, train_vecs = process_df(filename, data_y, data_x)\n",
    "\n",
    "def display_tweet():\n",
    "    # Change this for a html label\n",
    "    current_tweet_label.config(text=\"Tweet: %s\" % label_df.iloc[row]['text'])\n",
    "    \n",
    "def positive():\n",
    "\n",
    "def negative():\n",
    "    \n",
    "def useless():\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def retrain(n):\n",
    "    labelled_data = data.iloc[:n]\n",
    "    frames = [label, labelled_data]\n",
    "    new_data = pd.concat(frames)\n",
    "    training = [tuple(x) for x in new_data.values]\n",
    "    train_y = np.asarray([x[0] for x in training])\n",
    "    train_x = np.asarray([x[1] for x in training])\n",
    "    \n",
    "    # tokenize\n",
    "    tknzr.fit_on_texts(train_x)\n",
    "    tokenized_train_x = tknzr.texts_to_sequences(train_x)\n",
    "    #remove duplicate tokens\n",
    "    for i in range(0, len(tokenized_train_x)):\n",
    "        tokenized_train_x[i] = list(set(tokenized_train_x[i]))\n",
    "        \n",
    "    # training vectors\n",
    "    training_vectors = np.zeros((len(tokenized_train_x), max_words))\n",
    "    # create one-hot matrices out of the indexed tweets\n",
    "    for i in range(0, len(tokenized_train_x)):\n",
    "          training_vectors[i][tokenized_train_x[i]] = 1\n",
    "    \n",
    "    model = load_model('model.h5')\n",
    "    model.fit(training_vectors, train_y,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True)\n",
    "    \n",
    "    model_json = model.to_json()\n",
    "    with open('model.json', 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    model.save_weights('model.h5')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  1. ...,  0.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filename = \"old/5001labelled.csv\"\n",
    "#data_y = 'useful'\n",
    "#data_x = 'text'\n",
    "load_training_data(\"old/5001labelled.csv\", 'useful', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
