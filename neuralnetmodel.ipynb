{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3) Centra already supports Ethereum, Bitcoin, Centra, Dash, Litecoin, Zcash, and Monero. Nothing special about XRP here. 4) claiming this in anyway as a “pro” for XRP over other cryptos is disingenuous and mostly false.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"src/data_sets/training_set.csv\")\n",
    "label = df[['useful', 'text']]\n",
    "print(label.iloc[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '3 centra already supports ethereum bitcoin centra dash litecoin zcash monero nothing special xrp 4 claiming anyway pro xrp cryptos disingenuous mostly false')\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def format_text(s):\n",
    "    s = re.sub(r\"http\\S+\", \"\", s)\n",
    "    s = re.sub('[^0-9a-z #+_]', ' ', s.lower());\n",
    "    s = \" \".join(word for word in s.split() if word not in STOPWORDS)\n",
    "    return s\n",
    "\n",
    "label = label[label[\"text\"].notnull()]\n",
    "label.loc[:,\"text\"] = label.text.apply(lambda x: format_text(x))\n",
    "label.loc[:, \"text\"] = label.text.apply(lambda x : \" \".join(re.findall('[\\w]+'\n",
    "         ,x)))\n",
    "#label[\"text\"] = label[\"text\"].str.lower()\n",
    "training = [tuple(x) for x in label.values]\n",
    "\n",
    "print(training[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1128\n",
      "5370\n"
     ]
    }
   ],
   "source": [
    "# create our training data from the tweets\n",
    "train_x = np.asarray([x[1] for x in training])\n",
    "# index all the sentiment labels\n",
    "train_y = np.asarray([x[0] for x in training])\n",
    "\n",
    "# only work with the 3000 most popular words found in our dataset\n",
    "max_words = 10000\n",
    "\n",
    "# print(train_x[0])\n",
    "# print(train_y[0])\n",
    "\n",
    "useful_examples_index = np.where(train_y > 0)[0]\n",
    "number_of_useful_examples = len(useful_examples_index)\n",
    "useless_examples_index = np.where(train_y == 0)[0]\n",
    "number_of_useless_examples = len(useless_examples_index)\n",
    "\n",
    "print(number_of_useful_examples)\n",
    "print(number_of_useless_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Tokenizer\n",
    "tknzr = Tokenizer(lower=True, split=\" \", num_words=max_words)\n",
    "tknzr.fit_on_texts(train_x)\n",
    "\n",
    "#vocabulary:\n",
    "# print(tknzr.word_index)\n",
    "\n",
    "tokenized_train_x = tknzr.texts_to_sequences(train_x)\n",
    "\n",
    "#remove duplicate tokens\n",
    "for i in range(0, len(tokenized_train_x)):\n",
    "    tokenized_train_x[i] = list(set(tokenized_train_x[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  1. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "training_vectors = np.zeros((len(tokenized_train_x), max_words))\n",
    "# create one-hot matrices out of the indexed tweets\n",
    "for i in range(0, len(tokenized_train_x)):\n",
    "      training_vectors[i][tokenized_train_x[i]] = 1\n",
    "\n",
    "print(training_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4050 samples, validate on 450 samples\n",
      "Epoch 1/10\n",
      "4050/4050 [==============================] - 9s 2ms/step - loss: 0.4137 - acc: 0.8244 - val_loss: 0.3776 - val_acc: 0.8422\n",
      "Epoch 2/10\n",
      "4050/4050 [==============================] - 8s 2ms/step - loss: 0.2133 - acc: 0.9074 - val_loss: 0.3968 - val_acc: 0.8622\n",
      "Epoch 3/10\n",
      "4050/4050 [==============================] - 8s 2ms/step - loss: 0.0759 - acc: 0.9741 - val_loss: 0.5566 - val_acc: 0.8533\n",
      "Epoch 4/10\n",
      "4050/4050 [==============================] - 8s 2ms/step - loss: 0.0310 - acc: 0.9901 - val_loss: 0.5973 - val_acc: 0.8622\n",
      "Epoch 5/10\n",
      "4050/4050 [==============================] - 8s 2ms/step - loss: 0.0162 - acc: 0.9956 - val_loss: 0.6894 - val_acc: 0.8578\n",
      "Epoch 6/10\n",
      "4050/4050 [==============================] - 8s 2ms/step - loss: 0.0130 - acc: 0.9965 - val_loss: 0.7360 - val_acc: 0.8600\n",
      "Epoch 7/10\n",
      "4050/4050 [==============================] - 8s 2ms/step - loss: 0.0118 - acc: 0.9960 - val_loss: 0.7633 - val_acc: 0.8578\n",
      "Epoch 8/10\n",
      "4050/4050 [==============================] - 8s 2ms/step - loss: 0.0111 - acc: 0.9960 - val_loss: 0.8119 - val_acc: 0.8600\n",
      "Epoch 9/10\n",
      "4050/4050 [==============================] - 8s 2ms/step - loss: 0.0083 - acc: 0.9970 - val_loss: 0.7741 - val_acc: 0.8489\n",
      "Epoch 10/10\n",
      "4050/4050 [==============================] - 8s 2ms/step - loss: 0.0090 - acc: 0.9968 - val_loss: 0.7816 - val_acc: 0.8444\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# treat the labels as categories\n",
    "# train_y = keras.utils.to_categorical(train_y, 2)\n",
    "\n",
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=max_words, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# evaluate model with standardized dataset\n",
    "# estimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=1)\n",
    "# kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "# results = cross_val_score(estimator, training_vectors, train_y, cv=kfold)\n",
    "# print(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "\n",
    "\n",
    "model = create_baseline()\n",
    "\n",
    "model.fit(training_vectors[:4500], train_y[:4500],\n",
    "  batch_size=32,\n",
    "  epochs=10,\n",
    "  verbose=1,\n",
    "  validation_split=0.1,\n",
    "  shuffle=True)\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model on Test Set\n",
      "Number of spam tweets: 1636 Number of useful tweets: 362\n",
      "Accuracy on test set:  0.8638638638638638\n",
      "Identified 0.5359116022099447 of useful tweets\n",
      "Identified 0.9364303178484108 of spam tweets\n"
     ]
    }
   ],
   "source": [
    "#use completely unseen data (untrained)\n",
    "\n",
    "print(\"Running Model on Test Set\")\n",
    "\n",
    "prediction = model.predict(training_vectors[4500:]) \n",
    "actual_y = train_y[4500:]\n",
    "total = len(actual_y)\n",
    "correct = 0\n",
    "useful = 0\n",
    "spam = 0\n",
    "\n",
    "actual_useful = np.count_nonzero(actual_y)\n",
    "actual_spam = len(actual_y) - actual_useful\n",
    "\n",
    "print(\"Number of spam tweets: \" + str(actual_spam) + \" Number of useful tweets: \" + str(actual_useful))\n",
    "\n",
    "for p in range(0, len(prediction)):\n",
    "    predicted = round(prediction[p][0])\n",
    "    if predicted == actual_y[p]:\n",
    "        correct += 1\n",
    "        if predicted == 1:\n",
    "            useful += 1\n",
    "        if predicted == 0:\n",
    "            spam += 1\n",
    "        \n",
    "print(\"Accuracy on test set:  \" + str(correct/total))\n",
    "print(\"Identified \" + str(useful/actual_useful) + \" of useful tweets\")\n",
    "print(\"Identified \" + str(spam/actual_spam) + \" of spam tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
